{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import MobileNetSkipConcat\n",
    "device = 'cuda:3' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNetSkipConcat(3,pretrained=False)\n",
    "# model.load_state_dict(torch.load('fast_depth.pt'))\n",
    "# model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmyyoung/.local/lib/python3.5/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%0 : Float(1, 3, 256, 256)\n",
      "      %1 : Float(32, 3, 3, 3)\n",
      "      %2 : Float(32)\n",
      "      %3 : Float(32)\n",
      "      %4 : Float(32)\n",
      "      %5 : Float(32)\n",
      "      %6 : Long()\n",
      "      %7 : Float(32, 1, 3, 3)\n",
      "      %8 : Float(32)\n",
      "      %9 : Float(32)\n",
      "      %10 : Float(32)\n",
      "      %11 : Float(32)\n",
      "      %12 : Long()\n",
      "      %13 : Float(64, 32, 1, 1)\n",
      "      %14 : Float(64)\n",
      "      %15 : Float(64)\n",
      "      %16 : Float(64)\n",
      "      %17 : Float(64)\n",
      "      %18 : Long()\n",
      "      %19 : Float(64, 1, 3, 3)\n",
      "      %20 : Float(64)\n",
      "      %21 : Float(64)\n",
      "      %22 : Float(64)\n",
      "      %23 : Float(64)\n",
      "      %24 : Long()\n",
      "      %25 : Float(128, 64, 1, 1)\n",
      "      %26 : Float(128)\n",
      "      %27 : Float(128)\n",
      "      %28 : Float(128)\n",
      "      %29 : Float(128)\n",
      "      %30 : Long()\n",
      "      %31 : Float(128, 1, 3, 3)\n",
      "      %32 : Float(128)\n",
      "      %33 : Float(128)\n",
      "      %34 : Float(128)\n",
      "      %35 : Float(128)\n",
      "      %36 : Long()\n",
      "      %37 : Float(128, 128, 1, 1)\n",
      "      %38 : Float(128)\n",
      "      %39 : Float(128)\n",
      "      %40 : Float(128)\n",
      "      %41 : Float(128)\n",
      "      %42 : Long()\n",
      "      %43 : Float(128, 1, 3, 3)\n",
      "      %44 : Float(128)\n",
      "      %45 : Float(128)\n",
      "      %46 : Float(128)\n",
      "      %47 : Float(128)\n",
      "      %48 : Long()\n",
      "      %49 : Float(256, 128, 1, 1)\n",
      "      %50 : Float(256)\n",
      "      %51 : Float(256)\n",
      "      %52 : Float(256)\n",
      "      %53 : Float(256)\n",
      "      %54 : Long()\n",
      "      %55 : Float(256, 1, 3, 3)\n",
      "      %56 : Float(256)\n",
      "      %57 : Float(256)\n",
      "      %58 : Float(256)\n",
      "      %59 : Float(256)\n",
      "      %60 : Long()\n",
      "      %61 : Float(256, 256, 1, 1)\n",
      "      %62 : Float(256)\n",
      "      %63 : Float(256)\n",
      "      %64 : Float(256)\n",
      "      %65 : Float(256)\n",
      "      %66 : Long()\n",
      "      %67 : Float(256, 1, 3, 3)\n",
      "      %68 : Float(256)\n",
      "      %69 : Float(256)\n",
      "      %70 : Float(256)\n",
      "      %71 : Float(256)\n",
      "      %72 : Long()\n",
      "      %73 : Float(512, 256, 1, 1)\n",
      "      %74 : Float(512)\n",
      "      %75 : Float(512)\n",
      "      %76 : Float(512)\n",
      "      %77 : Float(512)\n",
      "      %78 : Long()\n",
      "      %79 : Float(512, 1, 3, 3)\n",
      "      %80 : Float(512)\n",
      "      %81 : Float(512)\n",
      "      %82 : Float(512)\n",
      "      %83 : Float(512)\n",
      "      %84 : Long()\n",
      "      %85 : Float(512, 512, 1, 1)\n",
      "      %86 : Float(512)\n",
      "      %87 : Float(512)\n",
      "      %88 : Float(512)\n",
      "      %89 : Float(512)\n",
      "      %90 : Long()\n",
      "      %91 : Float(512, 1, 3, 3)\n",
      "      %92 : Float(512)\n",
      "      %93 : Float(512)\n",
      "      %94 : Float(512)\n",
      "      %95 : Float(512)\n",
      "      %96 : Long()\n",
      "      %97 : Float(512, 512, 1, 1)\n",
      "      %98 : Float(512)\n",
      "      %99 : Float(512)\n",
      "      %100 : Float(512)\n",
      "      %101 : Float(512)\n",
      "      %102 : Long()\n",
      "      %103 : Float(512, 1, 3, 3)\n",
      "      %104 : Float(512)\n",
      "      %105 : Float(512)\n",
      "      %106 : Float(512)\n",
      "      %107 : Float(512)\n",
      "      %108 : Long()\n",
      "      %109 : Float(512, 512, 1, 1)\n",
      "      %110 : Float(512)\n",
      "      %111 : Float(512)\n",
      "      %112 : Float(512)\n",
      "      %113 : Float(512)\n",
      "      %114 : Long()\n",
      "      %115 : Float(512, 1, 3, 3)\n",
      "      %116 : Float(512)\n",
      "      %117 : Float(512)\n",
      "      %118 : Float(512)\n",
      "      %119 : Float(512)\n",
      "      %120 : Long()\n",
      "      %121 : Float(512, 512, 1, 1)\n",
      "      %122 : Float(512)\n",
      "      %123 : Float(512)\n",
      "      %124 : Float(512)\n",
      "      %125 : Float(512)\n",
      "      %126 : Long()\n",
      "      %127 : Float(512, 1, 3, 3)\n",
      "      %128 : Float(512)\n",
      "      %129 : Float(512)\n",
      "      %130 : Float(512)\n",
      "      %131 : Float(512)\n",
      "      %132 : Long()\n",
      "      %133 : Float(512, 512, 1, 1)\n",
      "      %134 : Float(512)\n",
      "      %135 : Float(512)\n",
      "      %136 : Float(512)\n",
      "      %137 : Float(512)\n",
      "      %138 : Long()\n",
      "      %139 : Float(512, 1, 3, 3)\n",
      "      %140 : Float(512)\n",
      "      %141 : Float(512)\n",
      "      %142 : Float(512)\n",
      "      %143 : Float(512)\n",
      "      %144 : Long()\n",
      "      %145 : Float(1024, 512, 1, 1)\n",
      "      %146 : Float(1024)\n",
      "      %147 : Float(1024)\n",
      "      %148 : Float(1024)\n",
      "      %149 : Float(1024)\n",
      "      %150 : Long()\n",
      "      %151 : Float(1024, 1, 3, 3)\n",
      "      %152 : Float(1024)\n",
      "      %153 : Float(1024)\n",
      "      %154 : Float(1024)\n",
      "      %155 : Float(1024)\n",
      "      %156 : Long()\n",
      "      %157 : Float(1024, 1024, 1, 1)\n",
      "      %158 : Float(1024)\n",
      "      %159 : Float(1024)\n",
      "      %160 : Float(1024)\n",
      "      %161 : Float(1024)\n",
      "      %162 : Long()\n",
      "      %163 : Float(1024, 1, 5, 5)\n",
      "      %164 : Float(1024)\n",
      "      %165 : Float(1024)\n",
      "      %166 : Float(1024)\n",
      "      %167 : Float(1024)\n",
      "      %168 : Long()\n",
      "      %169 : Float(512, 1024, 1, 1)\n",
      "      %170 : Float(512)\n",
      "      %171 : Float(512)\n",
      "      %172 : Float(512)\n",
      "      %173 : Float(512)\n",
      "      %174 : Long()\n",
      "      %175 : Float(512, 1, 5, 5)\n",
      "      %176 : Float(512)\n",
      "      %177 : Float(512)\n",
      "      %178 : Float(512)\n",
      "      %179 : Float(512)\n",
      "      %180 : Long()\n",
      "      %181 : Float(256, 512, 1, 1)\n",
      "      %182 : Float(256)\n",
      "      %183 : Float(256)\n",
      "      %184 : Float(256)\n",
      "      %185 : Float(256)\n",
      "      %186 : Long()\n",
      "      %187 : Float(512, 1, 5, 5)\n",
      "      %188 : Float(512)\n",
      "      %189 : Float(512)\n",
      "      %190 : Float(512)\n",
      "      %191 : Float(512)\n",
      "      %192 : Long()\n",
      "      %193 : Float(128, 512, 1, 1)\n",
      "      %194 : Float(128)\n",
      "      %195 : Float(128)\n",
      "      %196 : Float(128)\n",
      "      %197 : Float(128)\n",
      "      %198 : Long()\n",
      "      %199 : Float(256, 1, 5, 5)\n",
      "      %200 : Float(256)\n",
      "      %201 : Float(256)\n",
      "      %202 : Float(256)\n",
      "      %203 : Float(256)\n",
      "      %204 : Long()\n",
      "      %205 : Float(64, 256, 1, 1)\n",
      "      %206 : Float(64)\n",
      "      %207 : Float(64)\n",
      "      %208 : Float(64)\n",
      "      %209 : Float(64)\n",
      "      %210 : Long()\n",
      "      %211 : Float(128, 1, 5, 5)\n",
      "      %212 : Float(128)\n",
      "      %213 : Float(128)\n",
      "      %214 : Float(128)\n",
      "      %215 : Float(128)\n",
      "      %216 : Long()\n",
      "      %217 : Float(32, 128, 1, 1)\n",
      "      %218 : Float(32)\n",
      "      %219 : Float(32)\n",
      "      %220 : Float(32)\n",
      "      %221 : Float(32)\n",
      "      %222 : Long()\n",
      "      %223 : Float(3, 32, 1, 1)\n",
      "      %224 : Float(3)\n",
      "      %225 : Float(3)\n",
      "      %226 : Float(3)\n",
      "      %227 : Float(3)\n",
      "      %228 : Long()) {\n",
      "  %229 : Float(1, 32, 128, 128) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%0, %1), scope: MobileNetSkipConcat/Sequential[conv0]/Conv2d[0]\n",
      "  %230 : Float(1, 32, 128, 128) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%229, %2, %3, %4, %5), scope: MobileNetSkipConcat/Sequential[conv0]/BatchNorm2d[1]\n",
      "  %231 : Float(1, 32, 128, 128) = onnx::Clip[max=6, min=0](%230), scope: MobileNetSkipConcat/Sequential[conv0]/ReLU6[2]\n",
      "  %232 : Float(1, 32, 128, 128) = onnx::Conv[dilations=[1, 1], group=32, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%231, %7), scope: MobileNetSkipConcat/Sequential[conv1]/Conv2d[0]\n",
      "  %233 : Float(1, 32, 128, 128) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%232, %8, %9, %10, %11), scope: MobileNetSkipConcat/Sequential[conv1]/BatchNorm2d[1]\n",
      "  %234 : Float(1, 32, 128, 128) = onnx::Clip[max=6, min=0](%233), scope: MobileNetSkipConcat/Sequential[conv1]/ReLU6[2]\n",
      "  %235 : Float(1, 64, 128, 128) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%234, %13), scope: MobileNetSkipConcat/Sequential[conv1]/Conv2d[3]\n",
      "  %236 : Float(1, 64, 128, 128) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%235, %14, %15, %16, %17), scope: MobileNetSkipConcat/Sequential[conv1]/BatchNorm2d[4]\n",
      "  %237 : Float(1, 64, 128, 128) = onnx::Clip[max=6, min=0](%236), scope: MobileNetSkipConcat/Sequential[conv1]/ReLU6[5]\n",
      "  %238 : Float(1, 64, 64, 64) = onnx::Conv[dilations=[1, 1], group=64, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%237, %19), scope: MobileNetSkipConcat/Sequential[conv2]/Conv2d[0]\n",
      "  %239 : Float(1, 64, 64, 64) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%238, %20, %21, %22, %23), scope: MobileNetSkipConcat/Sequential[conv2]/BatchNorm2d[1]\n",
      "  %240 : Float(1, 64, 64, 64) = onnx::Clip[max=6, min=0](%239), scope: MobileNetSkipConcat/Sequential[conv2]/ReLU6[2]\n",
      "  %241 : Float(1, 128, 64, 64) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%240, %25), scope: MobileNetSkipConcat/Sequential[conv2]/Conv2d[3]\n",
      "  %242 : Float(1, 128, 64, 64) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%241, %26, %27, %28, %29), scope: MobileNetSkipConcat/Sequential[conv2]/BatchNorm2d[4]\n",
      "  %243 : Float(1, 128, 64, 64) = onnx::Clip[max=6, min=0](%242), scope: MobileNetSkipConcat/Sequential[conv2]/ReLU6[5]\n",
      "  %244 : Float(1, 128, 64, 64) = onnx::Conv[dilations=[1, 1], group=128, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%243, %31), scope: MobileNetSkipConcat/Sequential[conv3]/Conv2d[0]\n",
      "  %245 : Float(1, 128, 64, 64) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%244, %32, %33, %34, %35), scope: MobileNetSkipConcat/Sequential[conv3]/BatchNorm2d[1]\n",
      "  %246 : Float(1, 128, 64, 64) = onnx::Clip[max=6, min=0](%245), scope: MobileNetSkipConcat/Sequential[conv3]/ReLU6[2]\n",
      "  %247 : Float(1, 128, 64, 64) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%246, %37), scope: MobileNetSkipConcat/Sequential[conv3]/Conv2d[3]\n",
      "  %248 : Float(1, 128, 64, 64) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%247, %38, %39, %40, %41), scope: MobileNetSkipConcat/Sequential[conv3]/BatchNorm2d[4]\n",
      "  %249 : Float(1, 128, 64, 64) = onnx::Clip[max=6, min=0](%248), scope: MobileNetSkipConcat/Sequential[conv3]/ReLU6[5]\n",
      "  %250 : Float(1, 128, 32, 32) = onnx::Conv[dilations=[1, 1], group=128, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%249, %43), scope: MobileNetSkipConcat/Sequential[conv4]/Conv2d[0]\n",
      "  %251 : Float(1, 128, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%250, %44, %45, %46, %47), scope: MobileNetSkipConcat/Sequential[conv4]/BatchNorm2d[1]\n",
      "  %252 : Float(1, 128, 32, 32) = onnx::Clip[max=6, min=0](%251), scope: MobileNetSkipConcat/Sequential[conv4]/ReLU6[2]\n",
      "  %253 : Float(1, 256, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%252, %49), scope: MobileNetSkipConcat/Sequential[conv4]/Conv2d[3]\n",
      "  %254 : Float(1, 256, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%253, %50, %51, %52, %53), scope: MobileNetSkipConcat/Sequential[conv4]/BatchNorm2d[4]\n",
      "  %255 : Float(1, 256, 32, 32) = onnx::Clip[max=6, min=0](%254), scope: MobileNetSkipConcat/Sequential[conv4]/ReLU6[5]\n",
      "  %256 : Float(1, 256, 32, 32) = onnx::Conv[dilations=[1, 1], group=256, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%255, %55), scope: MobileNetSkipConcat/Sequential[conv5]/Conv2d[0]\n",
      "  %257 : Float(1, 256, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%256, %56, %57, %58, %59), scope: MobileNetSkipConcat/Sequential[conv5]/BatchNorm2d[1]\n",
      "  %258 : Float(1, 256, 32, 32) = onnx::Clip[max=6, min=0](%257), scope: MobileNetSkipConcat/Sequential[conv5]/ReLU6[2]\n",
      "  %259 : Float(1, 256, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%258, %61), scope: MobileNetSkipConcat/Sequential[conv5]/Conv2d[3]\n",
      "  %260 : Float(1, 256, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%259, %62, %63, %64, %65), scope: MobileNetSkipConcat/Sequential[conv5]/BatchNorm2d[4]\n",
      "  %261 : Float(1, 256, 32, 32) = onnx::Clip[max=6, min=0](%260), scope: MobileNetSkipConcat/Sequential[conv5]/ReLU6[5]\n",
      "  %262 : Float(1, 256, 16, 16) = onnx::Conv[dilations=[1, 1], group=256, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%261, %67), scope: MobileNetSkipConcat/Sequential[conv6]/Conv2d[0]\n",
      "  %263 : Float(1, 256, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%262, %68, %69, %70, %71), scope: MobileNetSkipConcat/Sequential[conv6]/BatchNorm2d[1]\n",
      "  %264 : Float(1, 256, 16, 16) = onnx::Clip[max=6, min=0](%263), scope: MobileNetSkipConcat/Sequential[conv6]/ReLU6[2]\n",
      "  %265 : Float(1, 512, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%264, %73), scope: MobileNetSkipConcat/Sequential[conv6]/Conv2d[3]\n",
      "  %266 : Float(1, 512, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%265, %74, %75, %76, %77), scope: MobileNetSkipConcat/Sequential[conv6]/BatchNorm2d[4]\n",
      "  %267 : Float(1, 512, 16, 16) = onnx::Clip[max=6, min=0](%266), scope: MobileNetSkipConcat/Sequential[conv6]/ReLU6[5]\n",
      "  %268 : Float(1, 512, 16, 16) = onnx::Conv[dilations=[1, 1], group=512, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%267, %79), scope: MobileNetSkipConcat/Sequential[conv7]/Conv2d[0]\n",
      "  %269 : Float(1, 512, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%268, %80, %81, %82, %83), scope: MobileNetSkipConcat/Sequential[conv7]/BatchNorm2d[1]\n",
      "  %270 : Float(1, 512, 16, 16) = onnx::Clip[max=6, min=0](%269), scope: MobileNetSkipConcat/Sequential[conv7]/ReLU6[2]\n",
      "  %271 : Float(1, 512, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%270, %85), scope: MobileNetSkipConcat/Sequential[conv7]/Conv2d[3]\n",
      "  %272 : Float(1, 512, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%271, %86, %87, %88, %89), scope: MobileNetSkipConcat/Sequential[conv7]/BatchNorm2d[4]\n",
      "  %273 : Float(1, 512, 16, 16) = onnx::Clip[max=6, min=0](%272), scope: MobileNetSkipConcat/Sequential[conv7]/ReLU6[5]\n",
      "  %274 : Float(1, 512, 16, 16) = onnx::Conv[dilations=[1, 1], group=512, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%273, %91), scope: MobileNetSkipConcat/Sequential[conv8]/Conv2d[0]\n",
      "  %275 : Float(1, 512, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%274, %92, %93, %94, %95), scope: MobileNetSkipConcat/Sequential[conv8]/BatchNorm2d[1]\n",
      "  %276 : Float(1, 512, 16, 16) = onnx::Clip[max=6, min=0](%275), scope: MobileNetSkipConcat/Sequential[conv8]/ReLU6[2]\n",
      "  %277 : Float(1, 512, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%276, %97), scope: MobileNetSkipConcat/Sequential[conv8]/Conv2d[3]\n",
      "  %278 : Float(1, 512, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%277, %98, %99, %100, %101), scope: MobileNetSkipConcat/Sequential[conv8]/BatchNorm2d[4]\n",
      "  %279 : Float(1, 512, 16, 16) = onnx::Clip[max=6, min=0](%278), scope: MobileNetSkipConcat/Sequential[conv8]/ReLU6[5]\n",
      "  %280 : Float(1, 512, 16, 16) = onnx::Conv[dilations=[1, 1], group=512, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%279, %103), scope: MobileNetSkipConcat/Sequential[conv9]/Conv2d[0]\n",
      "  %281 : Float(1, 512, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%280, %104, %105, %106, %107), scope: MobileNetSkipConcat/Sequential[conv9]/BatchNorm2d[1]\n",
      "  %282 : Float(1, 512, 16, 16) = onnx::Clip[max=6, min=0](%281), scope: MobileNetSkipConcat/Sequential[conv9]/ReLU6[2]\n",
      "  %283 : Float(1, 512, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%282, %109), scope: MobileNetSkipConcat/Sequential[conv9]/Conv2d[3]\n",
      "  %284 : Float(1, 512, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%283, %110, %111, %112, %113), scope: MobileNetSkipConcat/Sequential[conv9]/BatchNorm2d[4]\n",
      "  %285 : Float(1, 512, 16, 16) = onnx::Clip[max=6, min=0](%284), scope: MobileNetSkipConcat/Sequential[conv9]/ReLU6[5]\n",
      "  %286 : Float(1, 512, 16, 16) = onnx::Conv[dilations=[1, 1], group=512, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%285, %115), scope: MobileNetSkipConcat/Sequential[conv10]/Conv2d[0]\n",
      "  %287 : Float(1, 512, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%286, %116, %117, %118, %119), scope: MobileNetSkipConcat/Sequential[conv10]/BatchNorm2d[1]\n",
      "  %288 : Float(1, 512, 16, 16) = onnx::Clip[max=6, min=0](%287), scope: MobileNetSkipConcat/Sequential[conv10]/ReLU6[2]\n",
      "  %289 : Float(1, 512, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%288, %121), scope: MobileNetSkipConcat/Sequential[conv10]/Conv2d[3]\n",
      "  %290 : Float(1, 512, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%289, %122, %123, %124, %125), scope: MobileNetSkipConcat/Sequential[conv10]/BatchNorm2d[4]\n",
      "  %291 : Float(1, 512, 16, 16) = onnx::Clip[max=6, min=0](%290), scope: MobileNetSkipConcat/Sequential[conv10]/ReLU6[5]\n",
      "  %292 : Float(1, 512, 16, 16) = onnx::Conv[dilations=[1, 1], group=512, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%291, %127), scope: MobileNetSkipConcat/Sequential[conv11]/Conv2d[0]\n",
      "  %293 : Float(1, 512, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%292, %128, %129, %130, %131), scope: MobileNetSkipConcat/Sequential[conv11]/BatchNorm2d[1]\n",
      "  %294 : Float(1, 512, 16, 16) = onnx::Clip[max=6, min=0](%293), scope: MobileNetSkipConcat/Sequential[conv11]/ReLU6[2]\n",
      "  %295 : Float(1, 512, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%294, %133), scope: MobileNetSkipConcat/Sequential[conv11]/Conv2d[3]\n",
      "  %296 : Float(1, 512, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%295, %134, %135, %136, %137), scope: MobileNetSkipConcat/Sequential[conv11]/BatchNorm2d[4]\n",
      "  %297 : Float(1, 512, 16, 16) = onnx::Clip[max=6, min=0](%296), scope: MobileNetSkipConcat/Sequential[conv11]/ReLU6[5]\n",
      "  %298 : Float(1, 512, 8, 8) = onnx::Conv[dilations=[1, 1], group=512, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%297, %139), scope: MobileNetSkipConcat/Sequential[conv12]/Conv2d[0]\n",
      "  %299 : Float(1, 512, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%298, %140, %141, %142, %143), scope: MobileNetSkipConcat/Sequential[conv12]/BatchNorm2d[1]\n",
      "  %300 : Float(1, 512, 8, 8) = onnx::Clip[max=6, min=0](%299), scope: MobileNetSkipConcat/Sequential[conv12]/ReLU6[2]\n",
      "  %301 : Float(1, 1024, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%300, %145), scope: MobileNetSkipConcat/Sequential[conv12]/Conv2d[3]\n",
      "  %302 : Float(1, 1024, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%301, %146, %147, %148, %149), scope: MobileNetSkipConcat/Sequential[conv12]/BatchNorm2d[4]\n",
      "  %303 : Float(1, 1024, 8, 8) = onnx::Clip[max=6, min=0](%302), scope: MobileNetSkipConcat/Sequential[conv12]/ReLU6[5]\n",
      "  %304 : Float(1, 1024, 8, 8) = onnx::Conv[dilations=[1, 1], group=1024, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%303, %151), scope: MobileNetSkipConcat/Sequential[conv13]/Conv2d[0]\n",
      "  %305 : Float(1, 1024, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%304, %152, %153, %154, %155), scope: MobileNetSkipConcat/Sequential[conv13]/BatchNorm2d[1]\n",
      "  %306 : Float(1, 1024, 8, 8) = onnx::Clip[max=6, min=0](%305), scope: MobileNetSkipConcat/Sequential[conv13]/ReLU6[2]\n",
      "  %307 : Float(1, 1024, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%306, %157), scope: MobileNetSkipConcat/Sequential[conv13]/Conv2d[3]\n",
      "  %308 : Float(1, 1024, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%307, %158, %159, %160, %161), scope: MobileNetSkipConcat/Sequential[conv13]/BatchNorm2d[4]\n",
      "  %309 : Float(1, 1024, 8, 8) = onnx::Clip[max=6, min=0](%308), scope: MobileNetSkipConcat/Sequential[conv13]/ReLU6[5]\n",
      "  %310 : Float(1, 1024, 8, 8) = onnx::Conv[dilations=[1, 1], group=1024, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1]](%309, %163), scope: MobileNetSkipConcat/Sequential[decode_conv1]/Sequential[0]/Conv2d[0]\n",
      "  %311 : Float(1, 1024, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%310, %164, %165, %166, %167), scope: MobileNetSkipConcat/Sequential[decode_conv1]/Sequential[0]/BatchNorm2d[1]\n",
      "  %312 : Float(1, 1024, 8, 8) = onnx::Relu(%311), scope: MobileNetSkipConcat/Sequential[decode_conv1]/Sequential[0]/ReLU[2]\n",
      "  %313 : Float(1, 512, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%312, %169), scope: MobileNetSkipConcat/Sequential[decode_conv1]/Sequential[1]/Conv2d[0]\n",
      "  %314 : Float(1, 512, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%313, %170, %171, %172, %173), scope: MobileNetSkipConcat/Sequential[decode_conv1]/Sequential[1]/BatchNorm2d[1]\n",
      "  %315 : Float(1, 512, 8, 8) = onnx::Relu(%314), scope: MobileNetSkipConcat/Sequential[decode_conv1]/Sequential[1]/ReLU[2]\n",
      "  %316 : Tensor = onnx::Constant[value=-1  1 [ CPULongType{2} ]](), scope: MobileNetSkipConcat\n",
      "  %317 : Float(32768, 1) = onnx::Reshape(%315, %316), scope: MobileNetSkipConcat\n",
      "  %318 : Float(32768, 2) = onnx::Concat[axis=1](%317, %317), scope: MobileNetSkipConcat\n",
      "  %319 : Tensor = onnx::Constant[value= -1  16 [ CPULongType{2} ]](), scope: MobileNetSkipConcat\n",
      "  %320 : Float(4096, 16) = onnx::Reshape(%318, %319), scope: MobileNetSkipConcat\n",
      "  %321 : Float(4096, 32) = onnx::Concat[axis=1](%320, %320), scope: MobileNetSkipConcat\n",
      "  %322 : Tensor = onnx::Constant[value=  -1  512   16   16 [ CPULongType{4} ]](), scope: MobileNetSkipConcat\n",
      "  %323 : Float(1, 512, 16, 16) = onnx::Reshape(%321, %322), scope: MobileNetSkipConcat\n",
      "  %324 : Float(1, 512, 16, 16) = onnx::Conv[dilations=[1, 1], group=512, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1]](%323, %175), scope: MobileNetSkipConcat/Sequential[decode_conv2]/Sequential[0]/Conv2d[0]\n",
      "  %325 : Float(1, 512, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%324, %176, %177, %178, %179), scope: MobileNetSkipConcat/Sequential[decode_conv2]/Sequential[0]/BatchNorm2d[1]\n",
      "  %326 : Float(1, 512, 16, 16) = onnx::Relu(%325), scope: MobileNetSkipConcat/Sequential[decode_conv2]/Sequential[0]/ReLU[2]\n",
      "  %327 : Float(1, 256, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%326, %181), scope: MobileNetSkipConcat/Sequential[decode_conv2]/Sequential[1]/Conv2d[0]\n",
      "  %328 : Float(1, 256, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%327, %182, %183, %184, %185), scope: MobileNetSkipConcat/Sequential[decode_conv2]/Sequential[1]/BatchNorm2d[1]\n",
      "  %329 : Float(1, 256, 16, 16) = onnx::Relu(%328), scope: MobileNetSkipConcat/Sequential[decode_conv2]/Sequential[1]/ReLU[2]\n",
      "  %330 : Tensor = onnx::Constant[value=-1  1 [ CPULongType{2} ]](), scope: MobileNetSkipConcat\n",
      "  %331 : Float(65536, 1) = onnx::Reshape(%329, %330), scope: MobileNetSkipConcat\n",
      "  %332 : Float(65536, 2) = onnx::Concat[axis=1](%331, %331), scope: MobileNetSkipConcat\n",
      "  %333 : Tensor = onnx::Constant[value= -1  32 [ CPULongType{2} ]](), scope: MobileNetSkipConcat\n",
      "  %334 : Float(4096, 32) = onnx::Reshape(%332, %333), scope: MobileNetSkipConcat\n",
      "  %335 : Float(4096, 64) = onnx::Concat[axis=1](%334, %334), scope: MobileNetSkipConcat\n",
      "  %336 : Tensor = onnx::Constant[value=  -1  256   32   32 [ CPULongType{4} ]](), scope: MobileNetSkipConcat\n",
      "  %337 : Float(1, 256, 32, 32) = onnx::Reshape(%335, %336), scope: MobileNetSkipConcat\n",
      "  %338 : Float(1, 512, 32, 32) = onnx::Concat[axis=1](%337, %261), scope: MobileNetSkipConcat\n",
      "  %339 : Float(1, 512, 32, 32) = onnx::Conv[dilations=[1, 1], group=512, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1]](%338, %187), scope: MobileNetSkipConcat/Sequential[decode_conv3]/Sequential[0]/Conv2d[0]\n",
      "  %340 : Float(1, 512, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%339, %188, %189, %190, %191), scope: MobileNetSkipConcat/Sequential[decode_conv3]/Sequential[0]/BatchNorm2d[1]\n",
      "  %341 : Float(1, 512, 32, 32) = onnx::Relu(%340), scope: MobileNetSkipConcat/Sequential[decode_conv3]/Sequential[0]/ReLU[2]\n",
      "  %342 : Float(1, 128, 32, 32) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%341, %193), scope: MobileNetSkipConcat/Sequential[decode_conv3]/Sequential[1]/Conv2d[0]\n",
      "  %343 : Float(1, 128, 32, 32) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%342, %194, %195, %196, %197), scope: MobileNetSkipConcat/Sequential[decode_conv3]/Sequential[1]/BatchNorm2d[1]\n",
      "  %344 : Float(1, 128, 32, 32) = onnx::Relu(%343), scope: MobileNetSkipConcat/Sequential[decode_conv3]/Sequential[1]/ReLU[2]\n",
      "  %345 : Tensor = onnx::Constant[value=-1  1 [ CPULongType{2} ]](), scope: MobileNetSkipConcat\n",
      "  %346 : Float(131072, 1) = onnx::Reshape(%344, %345), scope: MobileNetSkipConcat\n",
      "  %347 : Float(131072, 2) = onnx::Concat[axis=1](%346, %346), scope: MobileNetSkipConcat\n",
      "  %348 : Tensor = onnx::Constant[value= -1  64 [ CPULongType{2} ]](), scope: MobileNetSkipConcat\n",
      "  %349 : Float(4096, 64) = onnx::Reshape(%347, %348), scope: MobileNetSkipConcat\n",
      "  %350 : Float(4096, 128) = onnx::Concat[axis=1](%349, %349), scope: MobileNetSkipConcat\n",
      "  %351 : Tensor = onnx::Constant[value=  -1  128   64   64 [ CPULongType{4} ]](), scope: MobileNetSkipConcat\n",
      "  %352 : Float(1, 128, 64, 64) = onnx::Reshape(%350, %351), scope: MobileNetSkipConcat\n",
      "  %353 : Float(1, 256, 64, 64) = onnx::Concat[axis=1](%352, %249), scope: MobileNetSkipConcat\n",
      "  %354 : Float(1, 256, 64, 64) = onnx::Conv[dilations=[1, 1], group=256, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1]](%353, %199), scope: MobileNetSkipConcat/Sequential[decode_conv4]/Sequential[0]/Conv2d[0]\n",
      "  %355 : Float(1, 256, 64, 64) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%354, %200, %201, %202, %203), scope: MobileNetSkipConcat/Sequential[decode_conv4]/Sequential[0]/BatchNorm2d[1]\n",
      "  %356 : Float(1, 256, 64, 64) = onnx::Relu(%355), scope: MobileNetSkipConcat/Sequential[decode_conv4]/Sequential[0]/ReLU[2]\n",
      "  %357 : Float(1, 64, 64, 64) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%356, %205), scope: MobileNetSkipConcat/Sequential[decode_conv4]/Sequential[1]/Conv2d[0]\n",
      "  %358 : Float(1, 64, 64, 64) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%357, %206, %207, %208, %209), scope: MobileNetSkipConcat/Sequential[decode_conv4]/Sequential[1]/BatchNorm2d[1]\n",
      "  %359 : Float(1, 64, 64, 64) = onnx::Relu(%358), scope: MobileNetSkipConcat/Sequential[decode_conv4]/Sequential[1]/ReLU[2]\n",
      "  %360 : Tensor = onnx::Constant[value=-1  1 [ CPULongType{2} ]](), scope: MobileNetSkipConcat\n",
      "  %361 : Float(262144, 1) = onnx::Reshape(%359, %360), scope: MobileNetSkipConcat\n",
      "  %362 : Float(262144, 2) = onnx::Concat[axis=1](%361, %361), scope: MobileNetSkipConcat\n",
      "  %363 : Tensor = onnx::Constant[value=  -1  128 [ CPULongType{2} ]](), scope: MobileNetSkipConcat\n",
      "  %364 : Float(4096, 128) = onnx::Reshape(%362, %363), scope: MobileNetSkipConcat\n",
      "  %365 : Float(4096, 256) = onnx::Concat[axis=1](%364, %364), scope: MobileNetSkipConcat\n",
      "  %366 : Tensor = onnx::Constant[value=  -1   64  128  128 [ CPULongType{4} ]](), scope: MobileNetSkipConcat\n",
      "  %367 : Float(1, 64, 128, 128) = onnx::Reshape(%365, %366), scope: MobileNetSkipConcat\n",
      "  %368 : Float(1, 128, 128, 128) = onnx::Concat[axis=1](%367, %237), scope: MobileNetSkipConcat\n",
      "  %369 : Float(1, 128, 128, 128) = onnx::Conv[dilations=[1, 1], group=128, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1]](%368, %211), scope: MobileNetSkipConcat/Sequential[decode_conv5]/Sequential[0]/Conv2d[0]\n",
      "  %370 : Float(1, 128, 128, 128) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%369, %212, %213, %214, %215), scope: MobileNetSkipConcat/Sequential[decode_conv5]/Sequential[0]/BatchNorm2d[1]\n",
      "  %371 : Float(1, 128, 128, 128) = onnx::Relu(%370), scope: MobileNetSkipConcat/Sequential[decode_conv5]/Sequential[0]/ReLU[2]\n",
      "  %372 : Float(1, 32, 128, 128) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%371, %217), scope: MobileNetSkipConcat/Sequential[decode_conv5]/Sequential[1]/Conv2d[0]\n",
      "  %373 : Float(1, 32, 128, 128) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%372, %218, %219, %220, %221), scope: MobileNetSkipConcat/Sequential[decode_conv5]/Sequential[1]/BatchNorm2d[1]\n",
      "  %374 : Float(1, 32, 128, 128) = onnx::Relu(%373), scope: MobileNetSkipConcat/Sequential[decode_conv5]/Sequential[1]/ReLU[2]\n",
      "  %375 : Tensor = onnx::Constant[value=-1  1 [ CPULongType{2} ]](), scope: MobileNetSkipConcat\n",
      "  %376 : Float(524288, 1) = onnx::Reshape(%374, %375), scope: MobileNetSkipConcat\n",
      "  %377 : Float(524288, 2) = onnx::Concat[axis=1](%376, %376), scope: MobileNetSkipConcat\n",
      "  %378 : Tensor = onnx::Constant[value=  -1  256 [ CPULongType{2} ]](), scope: MobileNetSkipConcat\n",
      "  %379 : Float(4096, 256) = onnx::Reshape(%377, %378), scope: MobileNetSkipConcat\n",
      "  %380 : Float(4096, 512) = onnx::Concat[axis=1](%379, %379), scope: MobileNetSkipConcat\n",
      "  %381 : Tensor = onnx::Constant[value=  -1   32  256  256 [ CPULongType{4} ]](), scope: MobileNetSkipConcat\n",
      "  %382 : Float(1, 32, 256, 256) = onnx::Reshape(%380, %381), scope: MobileNetSkipConcat\n",
      "  %383 : Float(1, 3, 256, 256) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%382, %223), scope: MobileNetSkipConcat/Sequential[decode_conv6]/Conv2d[0]\n",
      "  %384 : Float(1, 3, 256, 256) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%383, %224, %225, %226, %227), scope: MobileNetSkipConcat/Sequential[decode_conv6]/BatchNorm2d[1]\n",
      "  %385 : Float(1, 3, 256, 256) = onnx::Relu(%384), scope: MobileNetSkipConcat/Sequential[decode_conv6]/ReLU[2]\n",
      "  %386 : Float(1, 3, 256, 256) = onnx::Sigmoid(%385), scope: MobileNetSkipConcat\n",
      "  return (%386);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 256, 256, requires_grad=True)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model,             # model being run\n",
    "                x,                       # model input (or a tuple for multiple inputs)\n",
    "                \"fast_depth.onnx\", # where to save the model (can be a file or file-like object)\n",
    "                export_params=True,\n",
    "                 verbose=True)      # store the trained parameter weights inside the model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100/2/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 512, 16, 16]' is invalid for input of size 614400",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0929dab028b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m480\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI/Final/fast_depth/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupconstant\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[0;31m#             c,h,w = x.shape[1],x.shape[2],x.shape[3]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupsample_by_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI/Final/fast_depth/models.py\u001b[0m in \u001b[0;36mupsample_by_2\u001b[0;34m(x, c, h, w)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0mcc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcc1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcc1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 512, 16, 16]' is invalid for input of size 614400"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "a = model(torch.ones((1,3,640,480)).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model(torch.ones((1,3,640,480)).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
